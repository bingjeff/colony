{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.51 üöÄ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1080, 8111MiB)\n",
      "Setup complete ‚úÖ (8 CPUs, 31.3 GB RAM, 35.8/217.5 GB disk)\n"
     ]
    }
   ],
   "source": [
    "ultralytics.checks()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ultralytics.YOLO('models/yolov8s-blood-cell-detection.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.51 üöÄ Python-3.10.6 torch-1.13.1+cu117 CUDA:0 (NVIDIA GeForce GTX 1080, 8111MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=models/yolov8s-blood-cell-detection.pt, data=data/agar/agar.yaml, epochs=3, patience=50, batch=8, imgsz=480, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, min_memory=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs/detect/train14\n",
      "Overriding model.yaml nc=3 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.Conv                  [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.C2f                   [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.C2f                   [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.Conv                  [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.C2f                   [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.SPPF                  [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.C2f                   [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.Conv                  [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.C2f                   [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2118370  ultralytics.nn.modules.Detect                [6, [128, 256, 512]]          \n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks failed ‚ùå. Anomalies were detected with AMP on your system that may lead to NaN losses or zero-mAP results, so AMP will be disabled during training.\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/d/projects/colony/data/agar/dataset... 5476 images, 466 backgrounds, 4 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5476/5476 [00:02<00:00, 2200.13it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /mnt/d/projects/colony/data/agar/dataset/11988.jpg: ignoring corrupt image/label: negative label values [   -0.47082]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /mnt/d/projects/colony/data/agar/dataset/12160.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /mnt/d/projects/colony/data/agar/dataset/12269.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /mnt/d/projects/colony/data/agar/dataset/12316.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0007]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /mnt/d/projects/colony/data/agar/dataset/12607.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0059      1.0006]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /mnt/d/projects/colony/data/agar/dataset/13672.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /mnt/d/projects/colony/data/agar/dataset/15250.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1492]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /mnt/d/projects/colony/data/agar/dataset.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/d/projects/colony/data/agar/dataset... 1026 images, 78 backgrounds, 2 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1026/1026 [00:00<00:00, 2197.28it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /mnt/d/projects/colony/data/agar/dataset/12377.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0195]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /mnt/d/projects/colony/data/agar/dataset/12586.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0346       1.033      1.0707]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /mnt/d/projects/colony/data/agar/dataset.cache\n",
      "Plotting labels to runs/detect/train14/labels.jpg... \n",
      "Image sizes 480 train, 480 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train14\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3       6.5G      1.455      1.571      1.036        161        480: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 684/684 [05:28<00:00,  2.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:22<00:00,  2.84it/s]\n",
      "                   all       1024      22622       0.75      0.561      0.607      0.374\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/3      4.71G      1.237     0.8767     0.9327        233        480: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 684/684 [05:26<00:00,  2.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 64/64 [00:22<00:00,  2.82it/s]\n",
      "                   all       1024      22622      0.823       0.53      0.607      0.374\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/3      4.71G      1.179     0.7409     0.9185        131        480:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 346/684 [02:49<01:41,  3.32it/s]"
     ]
    }
   ],
   "source": [
    "# yolo train model=\"models/yolov8s-blood-cell-detection.pt\" data=\"data/agar/agar.yaml\" epochs=50 imgsz=480 batch=8\n",
    "model.train(data='data/agar/agar.yaml', imgsz=480, batch=8, epochs=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict('data/agar/dataset/12995.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = 'https://github.com/ultralytics/yolov5/raw/master/data/images/zidane.jpg'\n",
    "results = model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.cvtColor(results[0].plot(), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "# Setup the location for the AGAR dataset.\n",
    "DATA_PATH = 'data/agar/dataset/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine unique labels from dataset.\n",
    "label_set = set([])\n",
    "\n",
    "for c, file_path in enumerate(pathlib.Path(DATA_PATH).glob('*.json')):\n",
    "    if c % 1000 == 0:\n",
    "        print(f'Files opened: {c}')\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_dict = json.loads(f.read())\n",
    "        label_set.update(json_dict['classes'])\n",
    "\n",
    "with open(os.path.join(DATA_PATH, '../labels.txt')) as f:\n",
    "    for c, label in enumerate(label_set):\n",
    "        f.write(f'{c+1}:{label}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the class labels back from disk.\n",
    "\n",
    "class_labels = {}\n",
    "with open(os.path.join(DATA_PATH, '../labels.txt')) as f:\n",
    "    class_string = f.read()\n",
    "    for class_tuple in class_string.split('\\n'):\n",
    "        if class_tuple:\n",
    "            class_id, class_name = class_tuple.split(':')\n",
    "            class_labels[class_name] =  int(class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the JSON labels into TXT equivalent files.\n",
    "\n",
    "# Expected format for the label files is image000.txt with rows of:\n",
    "# class_number box1_x1_ratio box1_y1_ratio box1_width_ratio box1_height_ratio\n",
    "# The json records (x,y) as the ll_corner and txt encodes it as center.\n",
    "def convert_json_to_txt_label(label_dict, image_width, image_height, class_dict, default_class_id=0):\n",
    "    class_id = class_dict.get(label_dict['class']) or default_class_id\n",
    "    return [class_id, \n",
    "            (label_dict['x'] + 0.5 * label_dict['width']) / image_width,\n",
    "            (label_dict['y'] + 0.5 * label_dict['height']) / image_height,\n",
    "            label_dict['width'] / image_width,\n",
    "            label_dict['height'] / image_height,\n",
    "            ]\n",
    "\n",
    "for c in range(1,18000+1):\n",
    "    if c % 1000 == 0:\n",
    "        print(f'Processed {c}')\n",
    "    try:\n",
    "        im = Image.open(os.path.join(DATA_PATH, f'{c}.jpg'))\n",
    "        with open(os.path.join(DATA_PATH, f'{c}.json'), 'r') as f:\n",
    "            json_dict = json.loads(f.read())\n",
    "        with open(os.path.join(DATA_PATH, f'{c}.txt'), 'w') as f:\n",
    "            if 'labels' in json_dict:\n",
    "                for class_bbox in json_dict['labels']:\n",
    "                    id, x, y, w, h = convert_json_to_txt_label(class_bbox, im.width, im.height, class_labels)\n",
    "                    f.write(f'{id:d} {x} {y} {w} {h}\\n')\n",
    "    except Exception as err:\n",
    "        print(f'Image {c} failed.\\nERROR= {err=}\\nTYPE= {type(err)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 4001\n",
    "im = Image.open(os.path.join(DATA_PATH, f'{index}.jpg'))\n",
    "with open(os.path.join(DATA_PATH, f'{index}.json'), 'r') as f:\n",
    "    json_dict = json.loads(f.read())\n",
    "with open(os.path.join(DATA_PATH, f'{index}.txt'), 'r') as f:\n",
    "    bbox_raw = [[float(x) for x in line.split(' ') if x] for line in f.read().split('\\n') if line]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axes for the original JSON data.\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(im)\n",
    "ax.grid(visible=False)\n",
    "for bbox in json_dict['labels']:\n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle((bbox['x'], bbox['y']), bbox['width'], bbox['height'], linewidth=1, edgecolor='r', facecolor='none')\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "plt.show()\n",
    "\n",
    "# Create figure and axes for the new TXT data.\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(im)\n",
    "ax.grid(visible=False)\n",
    "for bbox in bbox_raw:\n",
    "    x = im.width * bbox[1]\n",
    "    y = im.height * bbox[2]\n",
    "    w = im.width * bbox[3]\n",
    "    h = im.height * bbox[4]\n",
    "    rect = patches.Rectangle((x-0.5*w, y-0.5*h), w, h, linewidth=1, edgecolor='b', facecolor='none')\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = '/media/bingjeff/8404A1F604A1EB7E/Users/bingj/Downloads/coco8/coco8/labels/train/000000000009.txt'\n",
    "image_file = '/media/bingjeff/8404A1F604A1EB7E/Users/bingj/Downloads/coco8/coco8/images/train/000000000009.jpg'\n",
    "im = Image.open(image_file)\n",
    "\n",
    "with open(label_file) as f:\n",
    "    bbox_raw = [[float(x) for x in line.split(' ') if x] for line in f.read().split('\\n') if line]\n",
    "\n",
    "\n",
    "# bowl    |45 0.479492 0.688771 0.955609 0.5955\n",
    "# bowl    |45 0.736516 0.247188 0.498875 0.476417\n",
    "# broccoli|50 0.637063 0.732938 0.494125 0.510583\n",
    "# bowl    |45 0.339438 0.418896 0.678875 0.7815\n",
    "# orange  |49 0.646836 0.132552 0.118047 0.0969375\n",
    "# orange  |49 0.773148 0.129802 0.0907344 0.0972292\n",
    "# orange  |49 0.668297 0.226906 0.131281 0.146896\n",
    "# orange  |49 0.642859 0.0792187 0.148063 0.148062\n",
    "\n",
    "# Create figure and axes for the new TXT data.\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(im)\n",
    "ax.grid(visible=False)\n",
    "for bbox in bbox_raw:\n",
    "    x = im.width * bbox[1]\n",
    "    y = im.height * bbox[2]\n",
    "    w = im.width * bbox[3]\n",
    "    h = im.height * bbox[4]\n",
    "    rect = patches.Rectangle((x-0.5*w, y-0.5*h), w, h, linewidth=1, edgecolor='b', facecolor='none')\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_raw[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colony-sSk4JcJw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
